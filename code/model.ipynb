{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "DnUZC8HEe44n",
    "outputId": "811cce69-d51b-4a40-c4ce-9709e6a4bb45"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "# %cd gdrive/My\\ Drive/Chinese-LSTM/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDo2EMlfe44r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import Tuple, List, Dict\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fsi63YibvWUV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.random import set_random_seed\n",
    "set_random_seed(42)\n",
    "from createdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "F2BKpKG0e442",
    "outputId": "11f1bc27-501e-4d11-fcce-0f9ab99f60ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****training*****\n",
      "creating file\n",
      "creating file\n",
      "X uni-bi shape: (105980, 30)(105980, 30)\n",
      "y shape: (105980, 30, 4)\n",
      "creating file\n",
      "creating file\n",
      "*****dev*****\n",
      "X uni-bi shape: (5930, 30)(5930, 30)\n",
      "y shape: (5930, 30, 4)\n"
     ]
    }
   ],
   "source": [
    "SUBSET = ['pku','msr']\n",
    "PADDING_SIZE = 30\n",
    "data, uni_word_to_idx, bi_word_to_idx = data_feed(SUBSET, PADDING_SIZE)\n",
    "\n",
    "# saving critical info to be later used when used on test sets\n",
    "predict_dict = dict()\n",
    "predict_dict.update({\"PADDING_SIZE\": PADDING_SIZE})\n",
    "predict_dict.update({\"UNIGRAM_DICT\": uni_word_to_idx})\n",
    "predict_dict.update({\"BIGRAM_DICT\": bi_word_to_idx})\n",
    "\n",
    "rel_path = '../resources/vocabs/'\n",
    "if not os.path.exists(rel_path):\n",
    "    os.mkdir(rel_path)\n",
    "with open(rel_path + '_'.join(SUBSET) + \"_\" + str(PADDING_SIZE) + '.json', 'w') as f:\n",
    "    json.dump(predict_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7jDoZtPe447"
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwiYx9rxe448"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = {\"unigrams\": data['train']['info']['uni_VocabSize'],\n",
    "              \"bigrams\": data['train']['info']['bi_VocabSize']}\n",
    "EMBEDDING_SIZE = {\"unigrams\": 64,\n",
    "                  \"bigrams\": 16}\n",
    "HIDDEN_SIZE = 256\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "LEARNING_RATE = 0.0015\n",
    "#INPUT_DROPOUT = 0.2\n",
    "LSTM_DROPOUT = 0.45\n",
    "RECURRENT_DROPOUT = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAL0xLWre45A"
   },
   "outputs": [],
   "source": [
    "K.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "VPMwCo_qe45D",
    "outputId": "d5f179c3-d7ff-4866-f915-3009ae657d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_unigrams (Embedding)  (None, None, 64)     343616      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_bigrams (Embedding)   (None, None, 16)     8430688     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenated (Concatenate)      (None, None, 80)     0           embedding_unigrams[0][0]         \n",
      "                                                                 embedding_bigrams[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Bi-directional_LSTM (Bidirectio (None, None, 512)    690176      concatenated[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 4)      2052        Bi-directional_LSTM[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 9,466,532\n",
      "Trainable params: 9,466,532\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_name = time.strftime('%Y-%m-%d_%H:%M:%S_%z')\n",
    "def Model_A(vocab_size, embedding_size, hidden_size, PADDING_SIZE, LEARNING_RATE, INPUT_DROPOUT, LSTM_DROPOUT, RECURRENT_DROPOUT):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    unigrams = K.layers.Input(shape=(None,))\n",
    "    embedding_unigrams = K.layers.Embedding(vocab_size[\"unigrams\"],\n",
    "                                            embedding_size['unigrams'],\n",
    "                                            mask_zero=True,\n",
    "                                            name = 'embedding_unigrams')(unigrams)\n",
    "    \n",
    "    bigrams = K.layers.Input(shape=(None,))\n",
    "    embedding_bigrams = K.layers.Embedding(vocab_size[\"bigrams\"],\n",
    "                                           embedding_size['bigrams'],\n",
    "                                           mask_zero=True,\n",
    "                                           name = 'embedding_bigrams')(bigrams)\n",
    "\n",
    "    merged_vector = K.layers.concatenate([embedding_unigrams, embedding_bigrams], axis=-1, name = 'concatenated')\n",
    "    \n",
    "    BI_LSTM = (K.layers.Bidirectional(\n",
    "               K.layers.LSTM(hidden_size, dropout = LSTM_DROPOUT,\n",
    "                             recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                             return_sequences=True,\n",
    "                             kernel_regularizer=K.regularizers.l2(0.01),\n",
    "                             activity_regularizer=K.regularizers.l1(0.01)\n",
    "                            ),name = 'Bi-directional_LSTM'))(merged_vector)\n",
    "    \n",
    "    predictions = K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax'))(BI_LSTM)\n",
    "    \n",
    "    model = K.models.Model(inputs=[unigrams, bigrams], outputs=predictions) \n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = K.optimizers.Adam(lr=LEARNING_RATE, clipnorm=1., clipvalue=0.5),\n",
    "                  metrics = ['acc', K.metrics.Precision()])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = Model_A(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE,\n",
    "                PADDING_SIZE, LEARNING_RATE, INPUT_DROPOUT,\n",
    "                LSTM_DROPOUT, RECURRENT_DROPOUT)\n",
    "# Let's print a summary of the model\n",
    "model.summary()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ua5XFu-Qe45G",
    "outputId": "898b3a27-1f75-4807-80c3-e036b29d4255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../resources/report_images'):\n",
    "    os.mkdir('../resources/report_images')\n",
    "\n",
    "cbk = K.callbacks.TensorBoard('../resources/logging/keras_model_'+model_name)\n",
    "print(\"\\nStarting training...\")\n",
    "K.utils.plot_model(model, to_file='../resources/report_images/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzQBdOH0e45J"
   },
   "outputs": [],
   "source": [
    "early_stopping = K.callbacks.EarlyStopping(monitor='val_precision',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=2, mode='auto')\n",
    "csv_logger = K.callbacks.CSVLogger('../resources/logging/keras_model_'+model_name+'.log')\n",
    "model_checkpoint = K.callbacks.ModelCheckpoint(filepath = '../resources/logging/keras_model_'+model_name+'.h5',\n",
    "                                               monitor='val_precision',\n",
    "                                               verbose=2,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "oMntKyxVe45L",
    "outputId": "8d8121d8-878b-4366-9eee-1c0c9085cb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105980 samples, validate on 5930 samples\n",
      "Epoch 1/20\n",
      "105856/105980 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.8481 - precision: 0.9184\n",
      "Epoch 00001: val_precision improved from inf to 0.91870, saving model to ../resources/logging/keras_model_2019-04-24_12:12:00_-0500.h5\n",
      "105980/105980 [==============================] - 569s 5ms/sample - loss: 0.4760 - acc: 0.8482 - precision: 0.9185 - val_loss: 0.3417 - val_acc: 0.9128 - val_precision: 0.9187\n",
      "Epoch 2/20\n",
      "105856/105980 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9484 - precision: 0.9537\n",
      "Epoch 00002: val_precision did not improve from 0.91870\n",
      "105980/105980 [==============================] - 640s 6ms/sample - loss: 0.1747 - acc: 0.9484 - precision: 0.9537 - val_loss: 0.3494 - val_acc: 0.9184 - val_precision: 0.9220\n",
      "Epoch 3/20\n",
      "105856/105980 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9598 - precision: 0.9627\n",
      "Epoch 00003: val_precision did not improve from 0.91870\n",
      "105980/105980 [==============================] - 736s 7ms/sample - loss: 0.1349 - acc: 0.9598 - precision: 0.9627 - val_loss: 0.3866 - val_acc: 0.9208 - val_precision: 0.9235\n",
      "Epoch 4/20\n",
      "105856/105980 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9656 - precision: 0.9676\n",
      "Epoch 00004: val_precision did not improve from 0.91870\n",
      "105980/105980 [==============================] - 632s 6ms/sample - loss: 0.1147 - acc: 0.9656 - precision: 0.9676 - val_loss: 0.4303 - val_acc: 0.9171 - val_precision: 0.9195\n",
      "Epoch 00004: early stopping\n",
      "Training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(data[\"train\"][\"X\"], data[\"train\"][\"y\"],\n",
    "          epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "          validation_data=(data[\"dev\"][\"X\"], data[\"dev\"][\"y\"]),\n",
    "          callbacks=[cbk, csv_logger, model_checkpoint, early_stopping]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YXVxZ5fe45O"
   },
   "outputs": [],
   "source": [
    "rel_path = '../resources/models'\n",
    "if not os.path.exists(rel_path):\n",
    "    os.mkdir(rel_path)\n",
    "weights = os.path.join(rel_path,'model_weights_'+model_name+'_'.join(SUBSET)+'.h5')\n",
    "model_name_save = os.path.join(rel_path,'model_'+model_name+'_'.join(SUBSET)+'.h5')\n",
    "model.save_weights(weights) #saving weights for further analysis\n",
    "model.save(model_name_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "S_Al2dE8e45V",
    "outputId": "625f0b15-28ce-45d6-be87-46d8b475b19c"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-71-2db29e4c7b13>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-2db29e4c7b13>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    axs[2].set_xlim(left=1, right = 4, 1)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def plot_training(model_name, save = False, PADDING_SIZE = None, epochs = None, subset = None, size = 30):\n",
    "    if not model_name:\n",
    "        selected = False\n",
    "        while not selected:\n",
    "            c=0\n",
    "            for ind, i in enumerate(os.listdir(\"../resources/logging\")):\n",
    "                print(\"{}:\\t{}\".format(ind, i))\n",
    "            x = int(input(\"Please select one of the above with a numer: \"))\n",
    "            model_name = os.listdir(\"../resources/logging\")[x]\n",
    "            if model_name.endswith(\".log\"):\n",
    "                selected = True\n",
    "            else: \n",
    "                print(\"please select a correct log file to load\")\n",
    "        history = pd.read_csv('../resources/logging/'+model_name)\n",
    "        #PADDING_SIZE, epochs, subset = 'Unknown', history.shape[0], 'Unknown'\n",
    "    else:\n",
    "        history = pd.read_csv('../resources/logging/keras_model_'+model_name+'.log')\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(10,7))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(history['acc'])\n",
    "    axs[0].plot(history['val_acc'])\n",
    "    #axs[0].title('model accuracy')\n",
    "    axs[0].set_ylabel('accuracy',size = size)\n",
    "    #axs[0].set_xlabel('epoch')\n",
    "    #axs[0].set_xticks((list(range(0,epochs))))\n",
    "    axs[0].legend(['train', 'dev'], loc='lower right')\n",
    "    axs[0].grid(alpha=0.7)\n",
    "\n",
    "    axs[0].set_title(\"Padding size = {}, epochs = {}, subset = {}\".format(PADDING_SIZE, epochs, subset), size = size)\n",
    "    \n",
    "    axs[0].tick_params(axis='x', labelsize=int(size/1.5))\n",
    "    axs[1].tick_params(axis='x', labelsize=int(size/1.5))\n",
    "    axs[2].tick_params(axis='x', labelsize=int(size/1.5))\n",
    "    axs[0].tick_params(axis='y', labelsize=int(size/1.5))\n",
    "    axs[1].tick_params(axis='y', labelsize=int(size/1.5))\n",
    "    axs[2].tick_params(axis='y', labelsize=int(size/1.5))\n",
    "    \n",
    "    # summarize history for loss\n",
    "    axs[1].plot(history['loss'])\n",
    "    axs[1].plot(history['val_loss'])\n",
    "    axs[1].set_ylabel('loss', size = size)\n",
    "    #axs[1].set_xlabel('epoch', size = size)\n",
    "    #axs[1].set_xticks((list(range(0,epochs))))\n",
    "    axs[1].legend(['train', 'dev'], loc='lower right')\n",
    "    axs[1].grid(alpha=0.7)\n",
    "    \n",
    "    axs[2].plot(history['precision'])\n",
    "    axs[2].plot(history['val_precision'])\n",
    "    axs[2].set_ylabel('Precision', size = size)\n",
    "    axs[2].set_xlabel('epoch', size = size)\n",
    "    #axs[2].set_xticks((list(range(0,epochs))))\n",
    "    axs[2].set_xlim(1,)\n",
    "    axs[2].legend(['train', 'dev'], loc='lower right')\n",
    "    axs[2].grid(alpha=0.7)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.show()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(wspace=1, hspace=0.1)\n",
    "    if save:\n",
    "        fig.savefig(\"../resources/report_images/\"+model_name+\"training_plot.png\")\n",
    "plot_training(model_name, True, PADDING_SIZE, 4, '-'.join(SUBSET), size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQBLVooVe45Y"
   },
   "outputs": [],
   "source": [
    "history = pd.read_csv('../resources/logging/keras_model_'+model_name+'.log') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.848176</td>\n",
       "      <td>0.475992</td>\n",
       "      <td>0.918468</td>\n",
       "      <td>0.912794</td>\n",
       "      <td>0.341734</td>\n",
       "      <td>0.918704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.174710</td>\n",
       "      <td>0.953714</td>\n",
       "      <td>0.918436</td>\n",
       "      <td>0.349393</td>\n",
       "      <td>0.922014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.959848</td>\n",
       "      <td>0.134948</td>\n",
       "      <td>0.962669</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>0.386564</td>\n",
       "      <td>0.923515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.965567</td>\n",
       "      <td>0.114664</td>\n",
       "      <td>0.967639</td>\n",
       "      <td>0.917146</td>\n",
       "      <td>0.430306</td>\n",
       "      <td>0.919472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch       acc      loss  precision   val_acc  val_loss  val_precision\n",
       "0      0  0.848176  0.475992   0.918468  0.912794  0.341734       0.918704\n",
       "1      1  0.948413  0.174710   0.953714  0.918436  0.349393       0.922014\n",
       "2      2  0.959848  0.134948   0.962669  0.920760  0.386564       0.923515\n",
       "3      3  0.965567  0.114664   0.967639  0.917146  0.430306       0.919472"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
