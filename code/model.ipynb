{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_Feed(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"training\":'../icwb2-data/training',\n",
    "             \"dev\":'../icwb2-data/gold',\n",
    "             \"testing\":'../icwb2-data/testing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(path, type_='LabelFile'):\n",
    "    x = []\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "            x.append(os.path.join(path, i))\n",
    "    return x\n",
    "\n",
    "Label_files = get_file_names(path = datasets['training'], type_ = 'LabelFile')\n",
    "Input_files = get_file_names(path = datasets['training'], type_ = 'InputFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../icwb2-data/training/msr_training_simplified_InputFile.utf8',\n",
       " '../icwb2-data/training/cityu_training_simplified_InputFile.utf8',\n",
       " '../icwb2-data/training/as_training_simplified_InputFile.utf8',\n",
       " '../icwb2-data/training/pku_training_simplified_InputFile.utf8']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "    \"\"\"\n",
    "    :param sentence Sentence as str\n",
    "    :type_: uni_grams or _bigrams\n",
    "    :return bigrams List of unigrams or bigrams\n",
    "    \"\"\"\n",
    "    n = 1 if type_ == 'uni_grams' else 2\n",
    "    grams = []\n",
    "    for i in range(len(sentence)-1):\n",
    "        gram = sentence[i:i+n]\n",
    "        grams.append(gram)\n",
    "    return grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_line = ''\n",
    "\n",
    "with open(Input_files[-1], 'r', encoding ='utf8') as f1:\n",
    "    for line in f1:\n",
    "        big_line+=line.rstrip()\n",
    "        \n",
    "final = split_into_grams(big_line, type_ = 'bi_grams') + split_into_grams(big_line, type_ = 'uni_grams')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = \"../icwb2-data/training/pku_training_simplified_InputFile_FEED.utf8\"\n",
    "with open(file_, 'w') as t:\n",
    "    for item in final:\n",
    "        t.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_1 = []\n",
    "with open(file_, 'r', encoding ='utf8') as f1:\n",
    "    for line in f1:\n",
    "        final_1.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {value:key for key,value in enumerate(vocab)}\n",
    "word_to_index['UNK'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vectors = []\n",
    "with open(Input_files[-1], 'r', encoding ='utf8') as f1:\n",
    "    for line in f1:\n",
    "        l = line.rstrip()\n",
    "        grams = split_into_grams(l, 'uni_grams') + split_into_grams(l,'bi_grams')\n",
    "        features_vectors.append([word_to_index[i] for i in grams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(label):\n",
    "    label = label.reshape(-1, 1)\n",
    "    enc = OneHotEncoder(categories='auto')\n",
    "    enc.fit(label)\n",
    "    return enc.transform(label).toarray()\n",
    "BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "with open(Label_files[-1], 'r', encoding ='utf8') as f1:\n",
    "    count = 0\n",
    "    for line in f1:\n",
    "        l = line.rstrip()\n",
    "        labels.append([BIES[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_BE_FOUND = 50 #length of longest line\n",
    "padded = pad_sequences(labels, maxlen=TO_BE_FOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = OHE(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(features_vectors, maxlen=TO_BE_FOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952800, 4)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "MAX_LENGTH = 80\n",
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 100\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    # remember to set mask_zero=True or the model consider the padding as a valid timestep!\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True))\n",
    "    #add a LSTM layer with some dropout in it\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "    # add a dense layer with sigmoid to get a probability value from 0.0 to 1.0\n",
    "    model.add(K.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          640000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 693,301\n",
      "Trainable params: 693,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 3\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 10\n",
    "size = int(len(X_train)/percent)\n",
    "X_train = X_train[:size]\n",
    "y_train = y_train[:size]\n",
    "dev_x = X_train[size:]\n",
    "dev_y = y_train[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95280, 4)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95280,)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (95280, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-ba0f6858cffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n\u001b[0;32m----> 3\u001b[0;31m           shuffle=True, validation_data=(dev_x, dev_y), callbacks=[cbk]) \n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2438\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2440\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            \u001b[0;34m'targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (95280, 4) was passed for an output of shape (None, 1) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(dev_x, dev_y), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stword = stopwords.words(\"german\")\n",
    "eng_stword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    ">>> segmenter = StanfordSegmenter(path_to_jar=”stanford-segmenter-3.4.1.jar”, path_to_sihan_corpora_dict=”./data”, path_to_model=”./data/pku.gz”, path_to_dict=”./data/dict-chris6.ser.gz”)\n",
    ">>> sentence = u”这是斯坦福中文分词器测试”\n",
    ">>> segmenter.segment(sentence)\n",
    ">>> u’\\u8fd9 \\u662f \\u65af\\u5766\\u798f \\u4e2d\\u6587 \\u5206\\u8bcd\\u5668 \\u6d4b\\u8bd5\\n’\n",
    ">>> segmenter.segment_file(“test.simp.utf8”)\n",
    ">>> u’\\u9762\\u5bf9 \\u65b0 \\u4e16\\u7eaa \\uff0c \\u4e16\\u754c \\u5404\\u56fd .\n",
    ">>> outfile = open(‘outfile’, ‘w’)\n",
    ">>> result = segmenter.segment(sentence)\n",
    ">>> outfile.write(result.encode(‘UTF-8′))\n",
    ">>> outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '这是斯坦福中文分词器测试'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from nltk.tokenize.stanford_segmenter import StanfordSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmenter = StanfordSegmenter()\n",
    "p = '../resources/stanford-chinese-corenlp-2018-10-05-models.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = StanfordSegmenter(path_to_jar=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter.segment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
