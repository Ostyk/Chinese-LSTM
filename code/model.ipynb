{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_Feed(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"training\":'../icwb2-data/training',\n",
    "             \"dev\":'../icwb2-data/gold',\n",
    "             \"testing\":'../icwb2-data/testing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(path, type_='LabelFile'):\n",
    "    x = []\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "            x.append(os.path.join(path, i))\n",
    "    return x\n",
    "\n",
    "Label_files = get_file_names(path = datasets['training'], type_ = 'LabelFile')\n",
    "Input_files = get_file_names(path = datasets['training'], type_ = 'InputFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following\n",
      "['msr', 'cityu', 'as', 'pku']\n",
      "\n",
      "msr\n",
      "cityu\n",
      "as\n",
      "pku\n",
      "Choose from the following\n",
      "['msr', 'cityu', 'as', 'pku']\n",
      "\n",
      "msr\n",
      "cityu\n",
      "as\n",
      "pku\n",
      "Choose from the following\n",
      "['msr', 'cityu', 'as', 'pku']\n",
      "cityu\n",
      "msr\n",
      "cityu\n",
      "CHOSEN cityu\n",
      "as\n",
      "CHOSEN cityu\n",
      "pku\n",
      "CHOSEN cityu\n",
      "../icwb2-data/training/cityu_training_simplified_LabelFile.utf8 ../icwb2-data/training/cityu_training_simplified_InputFile.utf8\n"
     ]
    }
   ],
   "source": [
    "names = ['msr','cityu','as','pku']\n",
    "choose = lambda i: i.split(\".utf8\")[0].split('/')[-1].split(\"_\")[0]\n",
    "\n",
    "e, r = False, False\n",
    "chosen = False\n",
    "while not chosen:\n",
    "    print(\"Choose from the following\")\n",
    "    print(names)\n",
    "    x = input(\"\")\n",
    "    for i in range(4):\n",
    "        print(choose(Input_files[i]))\n",
    "        if choose(Input_files[i]) == x: \n",
    "            Input_file = Input_files[i]\n",
    "            e = True\n",
    "        if choose(Label_files[i]) == x:\n",
    "            Label_file = Label_files[i]\n",
    "            r = True\n",
    "        if e and r:\n",
    "            chosen = True\n",
    "            print(\"CHOSEN {}\".format(x))\n",
    "    \n",
    "    \n",
    "print(Label_file, Input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "    \"\"\"\n",
    "    :param sentence Sentence as str\n",
    "    :type_: uni_grams or _bigrams\n",
    "    :return bigrams List of unigrams or bigrams\n",
    "    \"\"\"\n",
    "    n = 1 if type_ == 'uni_grams' else 2\n",
    "    grams = []\n",
    "    for i in range(len(sentence)-1):\n",
    "        gram = sentence[i:i+n]\n",
    "        grams.append(gram)\n",
    "    return grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_line = ''\n",
    "\n",
    "with open(Input_file, 'r', encoding ='utf8') as f1:\n",
    "    for line in f1:\n",
    "        big_line+=line.rstrip()\n",
    "        \n",
    "final = split_into_grams(big_line, type_ = 'bi_grams') + split_into_grams(big_line, type_ = 'uni_grams')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_ = \"../icwb2-data/training/pku_training_simplified_InputFile_FEED.utf8\"\n",
    "# with open(file_, 'w') as t:\n",
    "#     for item in final:\n",
    "#         t.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_1 = []\n",
    "# with open(file_, 'r', encoding ='utf8') as f1:\n",
    "#     for line in f1:\n",
    "#         final_1.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {value:key for key,value in enumerate(vocab)}\n",
    "word_to_index['UNK'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379343"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number assigned to a unigram or bigram in a sentence  based on the dictionary created earlier\n",
    "#one feature vector are the numbers from a sentence (line of the Input file)\n",
    "\n",
    "features_vectors = []\n",
    "with open(Input_file, 'r', encoding ='utf8') as f1:\n",
    "    for line in f1:\n",
    "        l = line.rstrip()\n",
    "        grams = split_into_grams(l, 'uni_grams') + split_into_grams(l,'bi_grams')\n",
    "\n",
    "        #difference is creating by grams line by line\n",
    "        features_vectors.append([word_to_index[i] for i in grams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(label):\n",
    "    label = label.reshape(-1, 1)\n",
    "    enc = OneHotEncoder(categories='auto')\n",
    "    enc.fit(label)\n",
    "    return enc.transform(label).toarray()\n",
    "BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#numerical BIES class given to a line \n",
    "labels = []\n",
    "with open(Label_file, 'r', encoding ='utf8') as f1:\n",
    "    count = 0\n",
    "    for line in f1:\n",
    "        l = line.rstrip()\n",
    "        labels.append([BIES[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAXLEN: 88\n"
     ]
    }
   ],
   "source": [
    "TO_BE_FOUND = int(np.mean([len(i) for i in features_vectors])) #length of longest line\n",
    "print(\"MAXLEN: {}\".format(TO_BE_FOUND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_labels = pad_sequences(labels, truncating='pre', padding='post', maxlen = TO_BE_FOUND)\n",
    "y =  K.utils.to_categorical(padded_labels, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(features_vectors, truncating='pre', padding='post', maxlen = TO_BE_FOUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_lines $\\times$ charecters per line (padded)  $\\times$ class from OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53019, 88, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_lines $\\times$ charecters per line (padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53019, 88)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "MAX_LENGTH = 88\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1029 \n",
    "\n",
    "Explains Timedistributed in many-to-many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TO_BE_FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, TO_BE_FOUND):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    # remember to set mask_zero=True or the model consider the padding as a valid timestep!\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = TO_BE_FOUND))\n",
    "    #add a LSTM layer with some dropout in it\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    # add a dense layer with sigmoid to get a probability value from 0.0 to 1.0s\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax')))\n",
    "    #time distribution\n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    #optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 88, 32)            12138976  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 88, 256)           295936    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 88, 4)             1028      \n",
      "=================================================================\n",
      "Total params: 12,435,940\n",
      "Trainable params: 12,435,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, TO_BE_FOUND)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 10\n",
    "size = int(len(X)/(1-percent))\n",
    "X_train = X[:2000]\n",
    "y_train = y[:2000]\n",
    "dev_x = X[-300:]\n",
    "dev_y = y[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (2000, 88)\n",
      "y train (2000, 88, 4)\n",
      "X dev (300, 88)\n",
      "y dev (300, 88, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train {}\".format(X_train.shape))\n",
    "print(\"y train {}\".format(y_train.shape))\n",
    "print(\"X dev {}\".format(dev_x.shape))\n",
    "print(\"y dev {}\".format(dev_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 31s 15ms/sample - loss: 1.1275 - acc: 0.5521 - val_loss: 1.1177 - val_acc: 0.5206\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 29s 14ms/sample - loss: 1.0500 - acc: 0.5613 - val_loss: 1.0499 - val_acc: 0.5442\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 29s 14ms/sample - loss: 0.9892 - acc: 0.5762 - val_loss: 1.0998 - val_acc: 0.5548\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 30s 15ms/sample - loss: 0.9459 - acc: 0.5854 - val_loss: 1.0313 - val_acc: 0.5549\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 28s 14ms/sample - loss: 0.9018 - acc: 0.5944 - val_loss: 1.0538 - val_acc: 0.5587\n",
      "Epoch 6/10\n",
      " 288/2000 [===>..........................] - ETA: 22s - loss: 0.8766 - acc: 0.6000"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(dev_x, dev_y), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
