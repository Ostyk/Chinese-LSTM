{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseDataset(set_type):\n",
    "    '''returns paths to Label and Input file for a specific dataset\n",
    "    args: set_type\n",
    "    return: Label_file, Input_file\n",
    "    '''\n",
    "    datasets = {\"training\":'../icwb2-data/training',\n",
    "                \"dev\":'../icwb2-data/gold',\n",
    "                \"testing\":'../icwb2-data/testing'}\n",
    "        \n",
    "    def get_file_names(path, type_='LabelFile'):\n",
    "        x = []\n",
    "        for i in os.listdir(path):\n",
    "            if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                x.append(os.path.join(path, i))\n",
    "        return x\n",
    "    Label_files = get_file_names(path = datasets[set_type], type_ = 'LabelFile')\n",
    "    Input_files = get_file_names(path = datasets[set_type], type_ = 'InputFile')\n",
    "    \n",
    "    names = ['msr','cityu','as','pku']\n",
    "    choose = lambda i: i.split(\".utf8\")[0].split('/')[-1].split(\"_\")[0]\n",
    "    \n",
    "    e, r = False, False\n",
    "    chosen = False\n",
    "    while not chosen:\n",
    "        print(\"Choose from the following\")\n",
    "        print(names)\n",
    "        x = input(\"\")\n",
    "        for i in range(4):\n",
    "            #print(choose(Input_files[i]))\n",
    "            if choose(Input_files[i]) == x: \n",
    "                Input_file = Input_files[i]\n",
    "                e = True\n",
    "            if choose(Label_files[i]) == x:\n",
    "                Label_file = Label_files[i]\n",
    "                r = True\n",
    "            if e and r:\n",
    "                chosen = True\n",
    "            \n",
    "    return Label_file, Input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self, LabelFile_path, InputFile_path):\n",
    "        self.Label_File = LabelFile_path\n",
    "        self.Input_File = InputFile_path\n",
    "    \n",
    "    def DateGen(self):\n",
    "        '''creates labels from the label file'''\n",
    "        \n",
    "        features_vectors = self.FeatureGenerator() \n",
    "        labels = self.BIESToNumerical()\n",
    "        \n",
    "        Optimal_Line_Length = int(np.mean([len(i) for i in features_vectors])) #length of longest line\n",
    "        \n",
    "        print(\"MAXLEN: {}\".format(Optimal_Line_Length)) \n",
    "        padded_labels = pad_sequences(labels, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        y =  K.utils.to_categorical(padded_labels, num_classes=4)\n",
    "        X = pad_sequences(features_vectors, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def BIESToNumerical(self):\n",
    "        '''Converts Label File from BIES encoding to numerical classes'''\n",
    "        BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}\n",
    "        #numerical BIES class given to a line \n",
    "        labels = []\n",
    "        with open(self.Label_File, 'r', encoding ='utf8') as f1:\n",
    "            count = 0\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                labels.append([BIES[i] for i in l])\n",
    "        return labels\n",
    "    \n",
    "    def FeatureGenerator(self):\n",
    "        '''Generates features based on '''\n",
    "        features_vectors = []\n",
    "        word_to_index = self.generateVocab()\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                grams = self.split_into_grams(l, 'uni_grams') + self.split_into_grams(l,'bi_grams')\n",
    "\n",
    "                #difference is creating by grams line by line\n",
    "                features_vectors.append([word_to_index[i] for i in grams])\n",
    "        \n",
    "        return features_vectors\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''\n",
    "        Generates vocabulary based on file\n",
    "        args: Inputfile, returns: word_to_index dict\n",
    "        '''\n",
    "        big_line = ''\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                big_line+=line.rstrip()\n",
    "        final = self.split_into_grams(big_line, type_ = 'bi_grams') + self.split_into_grams(big_line, type_ = 'uni_grams')\n",
    "        vocab = set(final)\n",
    "        print(len(vocab))\n",
    "        word_to_index = {value:key for key,value in enumerate(vocab)}\n",
    "        word_to_index['UNK'] = 0\n",
    "        \n",
    "        \n",
    "        return word_to_index\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "        \"\"\"\n",
    "        :param sentence Sentence as str\n",
    "        :type_: uni_grams or _bigrams\n",
    "        :return bigrams List of unigrams or bigrams\n",
    "        \"\"\"\n",
    "        n = 1 if type_ == 'uni_grams' else 2\n",
    "        grams = []\n",
    "        for i in range(len(sentence)-1):\n",
    "            gram = sentence[i:i+n]\n",
    "            grams.append(gram)\n",
    "        return grams\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following\n",
      "['msr', 'cityu', 'as', 'pku']\n",
      "pku\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following\n",
      "['msr', 'cityu', 'as', 'pku']\n",
      "pku\n",
      "285200\n",
      "MAXLEN: 189\n",
      "X shape: (19056, 189)\n",
      "y shape: (19056, 189, 4)\n"
     ]
    }
   ],
   "source": [
    "Label_file, Input_file = ChooseDataset(\"training\")\n",
    "A = CreateDataset(Label_file, Input_file)\n",
    "X, y = A.DateGen()\n",
    "print(\"X shape: {}\\ny shape: {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "MAX_LENGTH = 88\n",
    "VOCAB_SIZE = 285200\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "TO_BE_FOUND = 189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1029 \n",
    "\n",
    "Explains Timedistributed in many-to-many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, TO_BE_FOUND):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    # remember to set mask_zero=True or the model consider the padding as a valid timestep!\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = TO_BE_FOUND))\n",
    "    #add a LSTM layer with some dropout in it\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    # add a dense layer with sigmoid to get a probability value from 0.0 to 1.0s\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax')))\n",
    "    #time distribution\n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    #optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_10(vocab_size, embedding_size, hidden_size, TO_BE_FOUND):\n",
    "    print(\"Creating KERAS model\")\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    # remember to set mask_zero=True or the model consider the padding as a valid timestep!\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = TO_BE_FOUND))\n",
    "    #add a LSTM layer with some dropout in it\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    # add a dense layer with sigmoid to get a probability value from 0.0 to 1.0s\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax')))\n",
    "    #time distribution\n",
    "    # we are going to use the Adam optimizer which is a really powerful optimizer.\n",
    "    #optimizer = K.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TO_BE_FOUND' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f10d674648ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTO_BE_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let's print a summary of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TO_BE_FOUND' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, TO_BE_FOUND)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 10\n",
    "size = int(len(X)/(1-percent))\n",
    "X_train = X[:2000]\n",
    "y_train = y[:2000]\n",
    "dev_x = X[-300:]\n",
    "dev_y = y[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X train {}\".format(X_train.shape))\n",
    "print(\"y train {}\".format(y_train.shape))\n",
    "print(\"X dev {}\".format(dev_x.shape))\n",
    "print(\"y dev {}\".format(dev_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(dev_x, dev_y), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nEvaluating test...\")\n",
    "#loss_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "#print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
