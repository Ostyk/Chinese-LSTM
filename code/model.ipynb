{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.__version__ fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseDataset(set_type, subset):\n",
    "    '''returns paths to Label and Input file for a specific dataset\n",
    "    args: set_type\n",
    "    return: Label_file, Input_file\n",
    "    '''\n",
    "    datasets = {\"training\":'../icwb2-data/training',\n",
    "                \"dev\":'../icwb2-data/gold',\n",
    "                \"testing\":'../icwb2-data/testing'}\n",
    "        \n",
    "    def get_file_names(path, type_='LabelFile'):\n",
    "        x = []\n",
    "        dev = True if path.split(\"/\")[-1] == 'gold' else False #checks for dev\n",
    "        for i in os.listdir(path):\n",
    "            if dev and i.split(\"_\")[1][:4]=='test': #eliminates 'training' from 'gold' \n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "            elif not dev:\n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "        return x\n",
    "    \n",
    "    Label_files = get_file_names(path = datasets[set_type], type_ = 'LabelFile')\n",
    "    Input_files = get_file_names(path = datasets[set_type], type_ = 'InputFile')\n",
    "    names = ['msr','cityu','as','pku']\n",
    "    choose = lambda i: i.split(\".utf8\")[0].split('/')[-1].split(\"_\")[0]\n",
    "    e, r = False, False\n",
    "    chosen = False\n",
    "    while not chosen:\n",
    "        #x = input(\"Choose from the following: {}\".format(names))\n",
    "        x = subset\n",
    "        for i in range(len(Label_files)):\n",
    "            if choose(Input_files[i]) == x: \n",
    "                Input_file = Input_files[i]\n",
    "                e = True\n",
    "            if choose(Label_files[i]) == x:\n",
    "                Label_file = Label_files[i]\n",
    "                r = True\n",
    "            if e and r:\n",
    "                chosen = True\n",
    "            \n",
    "    return Label_file, Input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self, LabelFile_path, InputFile_path, PaddingSize, set_type, TrainingVocab):\n",
    "        self.Label_File = LabelFile_path\n",
    "        self.Input_File = InputFile_path\n",
    "        self.PaddingSize = PaddingSize\n",
    "        self.set_type = set_type\n",
    "        self.TrainingVocab = TrainingVocab\n",
    "    \n",
    "    def DateGen(self):\n",
    "        '''creates labels from the label file'''\n",
    "        \n",
    "        features_vectors, word_to_index = self.FeatureGenerator() \n",
    "        labels = self.BIESToNumerical()\n",
    "        #Optimal_Line_Length = int(np.mean([len(i) for i in features_vectors])) #length of longest line\n",
    "        Optimal_Line_Length = self.PaddingSize\n",
    "        \n",
    "        #print(\"MAXLEN: {}\".format(Optimal_Line_Length)) \n",
    "        padded_labels = pad_sequences(labels, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        y =  K.utils.to_categorical(padded_labels, num_classes=4)\n",
    "        X = pad_sequences(features_vectors, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        info = {\"MAXLEN\": Optimal_Line_Length,\n",
    "                \"VocabSize\": len(word_to_index)}\n",
    "        return X, y, info, word_to_index\n",
    "    \n",
    "    def BIESToNumerical(self):\n",
    "        '''Converts Label File from BIES encoding to numerical classes'''\n",
    "        BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}\n",
    "        #numerical BIES class given to a line \n",
    "        labels = []\n",
    "        with open(self.Label_File, 'r', encoding ='utf8') as f1:\n",
    "            count = 0\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                labels.append([BIES[i] for i in l])\n",
    "        return labels\n",
    "    \n",
    "    def FeatureGenerator(self):\n",
    "        '''Generates features based on '''\n",
    "        features_vectors = []\n",
    "        if self.set_type == 'training':\n",
    "            word_to_index = self.generateVocab()\n",
    "        else:\n",
    "            word_to_index = self.TrainingVocab\n",
    "\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                grams = self.split_into_grams(l, 'uni_grams') + self.split_into_grams(l,'bi_grams')\n",
    "                #difference is creating by grams line by line\n",
    "                features_vectors.append([word_to_index.get(i, 0) for i in grams])\n",
    "        return features_vectors, word_to_index\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''\n",
    "        Generates vocabulary based on file\n",
    "        args: Inputfile, returns: word_to_index dict\n",
    "        '''\n",
    "        big_line = ''\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                big_line+=line.rstrip()\n",
    "        final = self.split_into_grams(big_line, type_ = 'bi_grams') + self.split_into_grams(big_line, type_ = 'uni_grams')\n",
    "        vocab = set(final)\n",
    "        word_to_index = dict()\n",
    "        word_to_index['<UNK>'] = 0\n",
    "        word_to_index.update({value:key+1 for key,value in enumerate(vocab)})\n",
    "        \n",
    "        \n",
    "        return word_to_index\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "        \"\"\"\n",
    "        :param sentence Sentence as str\n",
    "        :type_: uni_grams or _bigrams\n",
    "        :return bigrams List of unigrams or bigrams\n",
    "        \"\"\"\n",
    "        n = 1 if type_ == 'uni_grams' else 2\n",
    "        grams = []\n",
    "        for i in range(len(sentence)-1):\n",
    "            gram = sentence[i:i+n]\n",
    "            grams.append(gram)\n",
    "        return grams\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(subset='pku',padding=50):\n",
    "    '''function that creates a dataset -- training, dev, and test\n",
    "    args: subset: any subset, padding: padding size\n",
    "    returns: (X_train, y_train), (X_dev, y_dev), (X_test, y_test), info_dev\n",
    "    '''\n",
    "    \n",
    "    type_ = \"training\"\n",
    "    print(type_)\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, None)\n",
    "    X_train, y_train, info_train, word_to_index_training = A.DateGen()\n",
    "    print(\"X shape: {}\\ny shape: {}\".format(X_train.shape, y_train.shape))\n",
    "    print(info_train)\n",
    "    \n",
    "    type_ = 'dev'\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, word_to_index_training)\n",
    "    X_dev, y_dev, info_dev, _ = A.DateGen()\n",
    "    print(type_)\n",
    "    print(\"X shape: {}\\ny shape: {}\".format(X_dev.shape, y_dev.shape))\n",
    "    print(info_dev)\n",
    "    \n",
    "    type_ = 'testing'\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, word_to_index_training)\n",
    "    X_test, y_test, info_test, _ = A.DateGen()\n",
    "    print(type_)\n",
    "    print(\"X shape: {}\\ny shape: {}\".format(X_test.shape, y_test.shape))\n",
    "    print(info_dev)\n",
    "    \n",
    "    return (X_train, y_train), (X_dev, y_dev), (X_test, y_test), info_dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "X shape: (19056, 50)\n",
      "y shape: (19056, 50, 4)\n",
      "{'MAXLEN': 50, 'VocabSize': 285201}\n",
      "dev\n",
      "X shape: (1945, 50)\n",
      "y shape: (1945, 50, 4)\n",
      "{'MAXLEN': 50, 'VocabSize': 285201}\n",
      "testing\n",
      "X shape: (1945, 50)\n",
      "y shape: (1945, 50, 4)\n",
      "{'MAXLEN': 50, 'VocabSize': 285201}\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_dev, y_dev), (X_test, y_test), info_dev = data(subset='pku',padding=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "VOCAB_SIZE = info_dev['VocabSize']\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "TO_BE_FOUND = info_dev['MAXLEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1029 \n",
    "\n",
    "Explains Timedistributed in many-to-many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, TO_BE_FOUND):\n",
    "    print(\"Creating KERAS model\")\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = TO_BE_FOUND))\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 32)            12139008  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 256)           295936    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 4)             1028      \n",
      "=================================================================\n",
      "Total params: 12,435,972\n",
      "Trainable params: 12,435,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, TO_BE_FOUND)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379343, 379336)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(i) for i in X_train]), max([max(i) for i in X_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.EarlyStopping at 0x188c49e10>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53019 samples, validate on 1493 samples\n",
      "Epoch 1/10\n",
      "53019/53019 [==============================] - 254s 5ms/sample - loss: 1.0758 - acc: 0.5143 - val_loss: 0.9124 - val_acc: 0.6056\n",
      "Epoch 2/10\n",
      "53019/53019 [==============================] - 227s 4ms/sample - loss: 0.8031 - acc: 0.6523 - val_loss: 0.8518 - val_acc: 0.6353\n",
      "Epoch 3/10\n",
      "53019/53019 [==============================] - 219s 4ms/sample - loss: 0.7172 - acc: 0.6914 - val_loss: 0.8086 - val_acc: 0.6617\n",
      "Epoch 4/10\n",
      "53019/53019 [==============================] - 236s 4ms/sample - loss: 0.6577 - acc: 0.7203 - val_loss: 0.7707 - val_acc: 0.6806\n",
      "Epoch 5/10\n",
      "53019/53019 [==============================] - 217s 4ms/sample - loss: 0.6156 - acc: 0.7385 - val_loss: 0.7851 - val_acc: 0.6701\n",
      "Epoch 6/10\n",
      "53019/53019 [==============================] - 235s 4ms/sample - loss: 0.5845 - acc: 0.7534 - val_loss: 0.7664 - val_acc: 0.6845\n",
      "Epoch 7/10\n",
      "53019/53019 [==============================] - 258s 5ms/sample - loss: 0.5576 - acc: 0.7668 - val_loss: 0.7773 - val_acc: 0.6721\n",
      "Epoch 8/10\n",
      "53019/53019 [==============================] - 252s 5ms/sample - loss: 0.5346 - acc: 0.7779 - val_loss: 0.7807 - val_acc: 0.6837\n",
      "Epoch 9/10\n",
      "53019/53019 [==============================] - 254s 5ms/sample - loss: 0.5126 - acc: 0.7883 - val_loss: 0.8093 - val_acc: 0.6815\n",
      "Epoch 10/10\n",
      "53019/53019 [==============================] - 223s 4ms/sample - loss: 0.4930 - acc: 0.7973 - val_loss: 0.8617 - val_acc: 0.6574\n",
      "Training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(X_dev, y_dev), callbacks=[cbk, csv_logger]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x188c4a320>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._get_callback_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5') #saving weights for further analysis\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test...\n",
      "Test data: loss = 4.348248  accuracy = 18.46% \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating test...\")\n",
    "loss_acc = model.evaluate(X_test, y_test, verbose=3)\n",
    "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "W0407 00:25:02.098168 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.114874 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.134521 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.193573 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.234097 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "TensorBoard 1.13.1 at http://Ostyks-MBP.lan:6006 (Press CTRL+C to quit)\n",
      "W0407 00:25:02.276803 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.291882 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.305765 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.326967 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.360383 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.393784 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.436908 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.482781 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.493369 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.530887 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.536523 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.553905 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.562463 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.583536 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.596323 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.625298 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.626715 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.640058 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.642275 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.662792 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.663898 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.674855 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.677443 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.695759 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.700196 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.766386 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.787030 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.814684 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.820303 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0407 00:25:02.918512 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0407 00:25:02.949526 123145567449088 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0407 00:25:02.969447 123145567449088 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5') #saving weights for further analysis\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1448/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-105-8fd4db36f2b7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-105-8fd4db36f2b7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    'Epoch 1/10\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
