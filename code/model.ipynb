{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.__version__ fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseDataset(set_type, subset):\n",
    "    '''returns paths to Label and Input file for a specific dataset\n",
    "    args: set_type\n",
    "    return: Label_file, Input_file\n",
    "    '''\n",
    "    datasets = {\"training\":'../icwb2-data/training',\n",
    "                \"dev\":'../icwb2-data/gold',\n",
    "                \"testing\":'../icwb2-data/testing'}\n",
    "\n",
    "    def get_file_names(path, type_='LabelFile'):\n",
    "        x = []\n",
    "        dev = True if path.split(\"/\")[-1] == 'gold' else False #checks for dev\n",
    "        for i in os.listdir(path):\n",
    "            if dev and i.split(\"_\")[1][:4]=='test': #eliminates 'training' from 'gold'\n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "            elif not dev:\n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "        return x\n",
    "\n",
    "    Label_files = get_file_names(path = datasets[set_type], type_ = 'LabelFile')\n",
    "    Input_files = get_file_names(path = datasets[set_type], type_ = 'InputFile')\n",
    "    names = ['msr','cityu','as','pku']\n",
    "    choose = lambda i: i.split(\".utf8\")[0].split('/')[-1].split(\"_\")[0]\n",
    "    e, r = False, False\n",
    "    chosen = False\n",
    "    while not chosen:\n",
    "        #x = input(\"Choose from the following: {}\".format(names))\n",
    "        x = subset\n",
    "        for i in range(len(Label_files)):\n",
    "            if choose(Input_files[i]) == x:\n",
    "                Input_file = Input_files[i]\n",
    "                e = True\n",
    "            if choose(Label_files[i]) == x:\n",
    "                Label_file = Label_files[i]\n",
    "                r = True\n",
    "            if e and r:\n",
    "                chosen = True\n",
    "\n",
    "    return Label_file, Input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self, LabelFile_path, InputFile_path, PaddingSize, set_type, TrainingVocab):\n",
    "        self.Label_File = LabelFile_path\n",
    "        self.Input_File = InputFile_path\n",
    "        self.PaddingSize = PaddingSize\n",
    "        self.set_type = set_type\n",
    "        self.TrainingVocab = TrainingVocab\n",
    "    \n",
    "    def DateGen(self):\n",
    "        '''creates labels from the label file'''\n",
    "        \n",
    "        uni_feature_vectors, bi_feature_vectors, uni_word_to_idx, bi_word_to_idx = self.FeatureGenerator() \n",
    "        \n",
    "        labels = self.BIESToNumerical()\n",
    "        Optimal_Line_Length = self.PaddingSize\n",
    "        \n",
    "        padded_labels = pad_sequences(labels, truncating='pre', padding='post', maxlen = self.PaddingSize)\n",
    "        y =  K.utils.to_categorical(padded_labels, num_classes=4)\n",
    "        \n",
    "        X_unigrams = pad_sequences(uni_feature_vectors, truncating='pre', padding='post', maxlen = self.PaddingSize)\n",
    "        X_bigrams = pad_sequences(bi_feature_vectors, truncating='pre', padding='post', maxlen = self.PaddingSize)\n",
    "        info = {\"MAXLEN\": Optimal_Line_Length,\n",
    "                \"uni_VocabSize\": len(uni_feature_vectors),\n",
    "                \"bi_VocabSize\": len(bi_feature_vectors)}\n",
    "        return X_unigrams, X_bigrams, y, info, uni_word_to_idx, bi_word_to_idx\n",
    "    \n",
    "    def BIESToNumerical(self):\n",
    "        '''Converts Label File from BIES encoding to numerical classes'''\n",
    "        BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}\n",
    "        #numerical BIES class given to a line \n",
    "        labels = []\n",
    "        with open(self.Label_File, 'r', encoding ='utf8') as f1:\n",
    "            count = 0\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                labels.append([BIES[i] for i in l])\n",
    "        return labels\n",
    "    \n",
    "    def FeatureGenerator(self):\n",
    "        '''Generates features based on unigrams and bigrams going line by line\n",
    "        returns: unigram_feature_vectors, bigram_feature_vectors\n",
    "        if training then returns also the word_to_idx for both unigrams and bigrams\n",
    "        '''\n",
    "        \n",
    "        uni_feature_vectors, bi_feature_vectors = [], []\n",
    "        \n",
    "        if self.set_type == 'training':\n",
    "            uni_word_to_idx, bi_word_to_idx = self.generateVocab()\n",
    "        else:\n",
    "            uni_word_to_idx, bi_word_to_idx = self.TrainingVocab\n",
    "\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                line = line.rstrip()\n",
    "                \n",
    "                unigrams = self.split_into_grams(line, 'uni_grams')\n",
    "                bigrams = self.split_into_grams(line,'bi_grams')\n",
    "                \n",
    "                uni_feature_vectors.append([uni_word_to_idx.get(i, 0) for i in unigrams])\n",
    "                bi_feature_vectors.append([bi_word_to_idx.get(i, 0) for i in bigrams])\n",
    "                \n",
    "        return uni_feature_vectors, bi_feature_vectors, uni_word_to_idx, bi_word_to_idx\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''\n",
    "        Generates vocabulary based on file\n",
    "        args: Inputfile, returns: word_to_index for unigrams and bigrams seperetly \n",
    "        '''\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            lines = f1.readlines()\n",
    "            raw = ' '.join(' '.join(lines).split()) #one long string\n",
    "        #creating unigrams and bigrams\n",
    "        unigrams, bigrams = self.split_into_grams(raw, 'uni_grams'), self.split_into_grams(raw, 'bi_grams')\n",
    "        del raw #erase from memory\n",
    "        #geting seperate vocavularies\n",
    "        unigrams_vocab, bigrams_vocab = set(unigrams), set(bigrams) \n",
    "        #print(len(unigrams_vocab), len(bigrams_vocab))\n",
    "        #initializing sepeate dictionaries\n",
    "        uni_word_to_idx, bi_word_to_idx = dict(), dict()\n",
    "        #Handling OOV\n",
    "        uni_word_to_idx[\"<UNK>\"], bi_word_to_idx[\"<UNK>\"] = 0, 0\n",
    "        #creating the rest of the word to index dict\n",
    "        uni_word_to_idx.update({value:key+1 for key,value in enumerate(unigrams_vocab)})\n",
    "        bi_word_to_idx.update({value:key+1 for key,value in enumerate(bigrams_vocab)})\n",
    "        \n",
    "        return uni_word_to_idx, bi_word_to_idx\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "        \"\"\"\n",
    "        :param sentence Sentence as str\n",
    "        :type_: uni_grams or _bigrams\n",
    "        :return bigrams List of unigrams or bigrams\n",
    "        \"\"\"\n",
    "        n = 1 if type_ == 'uni_grams' else 2\n",
    "        grams = []\n",
    "        for i in range(len(sentence)-1):\n",
    "            gram = sentence[i:i+n]\n",
    "            grams.append(gram)\n",
    "        return grams\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_feed(subset='pku',padding=50):\n",
    "    '''function that creates a dataset -- training, dev, and test\n",
    "    args: subset: any subset, padding: padding size\n",
    "    returns: (X_train, y_train), (X_dev, y_dev), (X_test, y_test), info_dev\n",
    "    '''\n",
    "    \n",
    "    type_ = \"training\"\n",
    "    print(\"*****{}*****\".format(type_))\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, None)\n",
    "    X_train_uni, X_train_bi, y_train, info_train, uni_word_to_idx, bi_word_to_idx = A.DateGen()\n",
    "    print(\"X uni-bi shape: {}{}\\ny shape: {}\".format(X_train_uni.shape, X_train_bi.shape, y_train.shape))\n",
    "    print(info_train)\n",
    "    \n",
    "    type_ = 'dev'\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    print(Label_file, Input_file)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, [uni_word_to_idx, bi_word_to_idx])\n",
    "    X_dev_uni, X_dev_bi, y_dev, info_dev, _, _ = A.DateGen()\n",
    "    print(\"*****{}*****\".format(type_))\n",
    "    print(\"X uni-bi shape: {}{}\\ny shape: {}\".format(X_dev_uni.shape, X_dev_bi.shape, y_train.shape))\n",
    "    print(info_dev)\n",
    "    \n",
    "    type_ = 'testing'\n",
    "    Label_file, Input_file = ChooseDataset(type_, subset)\n",
    "    A = CreateDataset(Label_file, Input_file, padding, type_, [uni_word_to_idx, bi_word_to_idx])\n",
    "    X_test_uni, X_test_bi, y_test, info_test, _, _ = A.DateGen()\n",
    "    print(\"*****{}*****\".format(type_))\n",
    "    print(\"X uni-bi shape: {}{}\\ny shape: {}\".format(X_test_uni.shape, X_test_bi.shape, y_train.shape))\n",
    "    print(info_test)\n",
    "    \n",
    "    return {\"train\": {\"X\": [X_train_uni, X_train_bi],\n",
    "                      \"y\": y_train}, \n",
    "           \"dev\": {\"X\": [X_dev_uni, X_dev_bi],\n",
    "                   \"y\": y_dev},\n",
    "           \"test\": {\"X\": [X_test_uni, X_test_bi],\n",
    "                    \"y\": y_test},\n",
    "           \"info\": info_dev}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****training*****\n",
      "X uni-bi shape: (19056, 10)(19056, 10)\n",
      "y shape: (19056, 10, 4)\n",
      "{'MAXLEN': 10, 'uni_VocabSize': 19056, 'bi_VocabSize': 19056}\n",
      "../icwb2-data/gold/pku_test_gold_simplified_LabelFile.utf8 ../icwb2-data/gold/pku_test_gold_simplified_InputFile.utf8\n",
      "*****dev*****\n",
      "X uni-bi shape: (1945, 10)(1945, 10)\n",
      "y shape: (19056, 10, 4)\n",
      "{'MAXLEN': 10, 'uni_VocabSize': 1945, 'bi_VocabSize': 1945}\n",
      "*****testing*****\n",
      "X uni-bi shape: (1945, 10)(1945, 10)\n",
      "y shape: (19056, 10, 4)\n",
      "{'MAXLEN': 10, 'uni_VocabSize': 1945, 'bi_VocabSize': 1945}\n"
     ]
    }
   ],
   "source": [
    "data = data_feed(subset='pku',padding=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "VOCAB_SIZE = 5000#info_dev['uni_VocabSize']\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "PADDING_SIZE = data['info']['MAXLEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1029 \n",
    "\n",
    "Explains Timedistributed in many-to-many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, PADDING_SIZE):\n",
    "    print(\"Creating KERAS model\")\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = PADDING_SIZE))\n",
    "    \n",
    "    #concatenate\n",
    "    model.add(K.layers.Bidirectional(\n",
    "              K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(K.layers.TimeDistributed(\n",
    "              K.layers.Dense(4, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 32)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 512)           591872    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 4)             2052      \n",
      "=================================================================\n",
      "Total params: 753,924\n",
      "Trainable params: 753,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, PADDING_SIZE)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19056 samples, validate on 1945 samples\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "19056/19056 [==============================] - 24s 1ms/sample - loss: 1.0556 - acc: 0.5695 - val_loss: 0.9760 - val_acc: 0.5815\n",
      "Epoch 2/10\n",
      "19056/19056 [==============================] - 22s 1ms/sample - loss: 0.9357 - acc: 0.6011 - val_loss: 0.8586 - val_acc: 0.6367\n",
      "Epoch 3/10\n",
      "19056/19056 [==============================] - 23s 1ms/sample - loss: 0.7776 - acc: 0.6910 - val_loss: 0.7071 - val_acc: 0.7276\n",
      "Epoch 4/10\n",
      "19056/19056 [==============================] - 24s 1ms/sample - loss: 0.6698 - acc: 0.7432 - val_loss: 0.6483 - val_acc: 0.7426\n",
      "Epoch 5/10\n",
      "19056/19056 [==============================] - 28s 1ms/sample - loss: 0.6133 - acc: 0.7639 - val_loss: 0.6302 - val_acc: 0.7530\n",
      "Epoch 6/10\n",
      "19056/19056 [==============================] - 22s 1ms/sample - loss: 0.5765 - acc: 0.7796 - val_loss: 0.6220 - val_acc: 0.7540\n",
      "Epoch 7/10\n",
      "19056/19056 [==============================] - 22s 1ms/sample - loss: 0.5481 - acc: 0.7894 - val_loss: 0.6142 - val_acc: 0.7509\n",
      "Epoch 8/10\n",
      "19056/19056 [==============================] - 22s 1ms/sample - loss: 0.5234 - acc: 0.8002 - val_loss: 0.6212 - val_acc: 0.7546\n",
      "Epoch 9/10\n",
      "19056/19056 [==============================] - 22s 1ms/sample - loss: 0.5016 - acc: 0.8092 - val_loss: 0.6118 - val_acc: 0.7571\n",
      "Epoch 10/10\n",
      "19056/19056 [==============================] - 24s 1ms/sample - loss: 0.4810 - acc: 0.8178 - val_loss: 0.6270 - val_acc: 0.7541\n",
      "Training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(data[\"train\"][\"X\"], data[\"train\"][\"y\"], epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(data[\"dev\"][\"X\"], data[\"dev\"][\"y\"]), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._get_callback_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5') #saving weights for further analysis\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test...\n",
      "Test data: loss = 4.975396  accuracy = 3.67% \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating test...\")\n",
    "loss_acc = model.evaluate(data[\"test\"][\"X\"], data[\"test\"][\"y\"], verbose=3)\n",
    "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('my_model_weights.h5') #saving weights for further analysis\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1448/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
