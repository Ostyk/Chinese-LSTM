{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed libraries\n",
    "- Tensorflow (compatible with 1.12.x)\n",
    "- Numpy\n",
    "- Sklearn\n",
    "- nltk\n",
    "- Maplotlib\n",
    "- gensim\n",
    "- All the standard libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences, TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.__version__ fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseDataset(set_type):\n",
    "    '''returns paths to Label and Input file for a specific dataset\n",
    "    args: set_type\n",
    "    return: Label_file, Input_file\n",
    "    '''\n",
    "    datasets = {\"training\":'../icwb2-data/training',\n",
    "                \"dev\":'../icwb2-data/gold',\n",
    "                \"testing\":'../icwb2-data/testing'}\n",
    "        \n",
    "    def get_file_names(path, type_='LabelFile'):\n",
    "        x = []\n",
    "        dev = True if path.split(\"/\")[-1] == 'gold' else False #checks for dev\n",
    "        for i in os.listdir(path):\n",
    "            if dev and i.split(\"_\")[1][:4]=='test': #eliminates 'training' from 'gold' \n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "            elif not dev:\n",
    "                if os.path.splitext(i)[0].split(\"_\")[-1] == type_:\n",
    "                    x.append(os.path.join(path, i))\n",
    "        return x\n",
    "    \n",
    "    Label_files = get_file_names(path = datasets[set_type], type_ = 'LabelFile')\n",
    "    Input_files = get_file_names(path = datasets[set_type], type_ = 'InputFile')\n",
    "    names = ['msr','cityu','as','pku']\n",
    "    choose = lambda i: i.split(\".utf8\")[0].split('/')[-1].split(\"_\")[0]\n",
    "    e, r = False, False\n",
    "    chosen = False\n",
    "    while not chosen:\n",
    "        x = input(\"Choose from the following: {}\".format(names))\n",
    "        for i in range(len(Label_files)):\n",
    "            if choose(Input_files[i]) == x: \n",
    "                Input_file = Input_files[i]\n",
    "                e = True\n",
    "            if choose(Label_files[i]) == x:\n",
    "                Label_file = Label_files[i]\n",
    "                r = True\n",
    "            if e and r:\n",
    "                chosen = True\n",
    "            \n",
    "    return Label_file, Input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(object):\n",
    "    '''makes feed files of combined unigrams and bigrams'''\n",
    "    def __init__(self, LabelFile_path, InputFile_path, PaddingSize, set_type, TrainingVocab):\n",
    "        self.Label_File = LabelFile_path\n",
    "        self.Input_File = InputFile_path\n",
    "        self.PaddingSize = PaddingSize\n",
    "        self.set_type = set_type\n",
    "        self.TrainingVocab = TrainingVocab\n",
    "    \n",
    "    def DateGen(self):\n",
    "        '''creates labels from the label file'''\n",
    "        \n",
    "        features_vectors, word_to_index = self.FeatureGenerator() \n",
    "        labels = self.BIESToNumerical()\n",
    "        #Optimal_Line_Length = int(np.mean([len(i) for i in features_vectors])) #length of longest line\n",
    "        Optimal_Line_Length = self.PaddingSize\n",
    "        \n",
    "        #print(\"MAXLEN: {}\".format(Optimal_Line_Length)) \n",
    "        padded_labels = pad_sequences(labels, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        y =  K.utils.to_categorical(padded_labels, num_classes=4)\n",
    "        X = pad_sequences(features_vectors, truncating='pre', padding='post', maxlen = Optimal_Line_Length)\n",
    "        \n",
    "        info = {\"MAXLEN\": Optimal_Line_Length,\n",
    "                \"VocabSize\": len(word_to_index)}\n",
    "        return X, y, info, word_to_index\n",
    "    \n",
    "    def BIESToNumerical(self):\n",
    "        '''Converts Label File from BIES encoding to numerical classes'''\n",
    "        BIES = {'B' : 0, 'I' : 1, 'E' : 2, 'S' : 3}\n",
    "        #numerical BIES class given to a line \n",
    "        labels = []\n",
    "        with open(self.Label_File, 'r', encoding ='utf8') as f1:\n",
    "            count = 0\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                labels.append([BIES[i] for i in l])\n",
    "        return labels\n",
    "    \n",
    "    def FeatureGenerator(self):\n",
    "        '''Generates features based on '''\n",
    "        features_vectors = []\n",
    "        if self.set_type == 'training':\n",
    "            word_to_index = self.generateVocab()\n",
    "        else:\n",
    "            word_to_index = self.TrainingVocab\n",
    "\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                l = line.rstrip()\n",
    "                grams = self.split_into_grams(l, 'uni_grams') + self.split_into_grams(l,'bi_grams')\n",
    "                #difference is creating by grams line by line\n",
    "#                 c=0\n",
    "#                 if not self.set_type =='training':\n",
    "#                     for q in grams:\n",
    "#                         print(q, word_to_index_training.get(q))\n",
    "#                         break\n",
    "#                         if word_to_index_training.get(q) is None:\n",
    "#                             q = '<UNK>'\n",
    "#                             c+=1\n",
    "#                         features_vectors.append(word_to_index[q])\n",
    "#                 else:\n",
    "                features_vectors.append([word_to_index.get(i, 0) for i in grams])\n",
    "            #print(\"unknown words: {}\\total: {}\".format(c, len(word_to_index)))\n",
    "        return features_vectors, word_to_index\n",
    "    \n",
    "    def generateVocab(self):\n",
    "        '''\n",
    "        Generates vocabulary based on file\n",
    "        args: Inputfile, returns: word_to_index dict\n",
    "        '''\n",
    "        big_line = ''\n",
    "        with open(self.Input_File, 'r', encoding ='utf8') as f1:\n",
    "            for line in f1:\n",
    "                big_line+=line.rstrip()\n",
    "        final = self.split_into_grams(big_line, type_ = 'bi_grams') + self.split_into_grams(big_line, type_ = 'uni_grams')\n",
    "        vocab = set(final)\n",
    "        word_to_index = dict()\n",
    "        word_to_index['<UNK>'] = 0\n",
    "        word_to_index.update({value:key+1 for key,value in enumerate(vocab)})\n",
    "        \n",
    "        \n",
    "        return word_to_index\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_grams(sentence: str, type_ = 'uni_grams') -> List[str]:\n",
    "        \"\"\"\n",
    "        :param sentence Sentence as str\n",
    "        :type_: uni_grams or _bigrams\n",
    "        :return bigrams List of unigrams or bigrams\n",
    "        \"\"\"\n",
    "        n = 1 if type_ == 'uni_grams' else 2\n",
    "        grams = []\n",
    "        for i in range(len(sentence)-1):\n",
    "            gram = sentence[i:i+n]\n",
    "            grams.append(gram)\n",
    "        return grams\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following: ['msr', 'cityu', 'as', 'pku']pku\n",
      "X shape: (19056, 10)\n",
      "y shape: (19056, 10, 4)\n",
      "{'MAXLEN': 10, 'VocabSize': 285201}\n"
     ]
    }
   ],
   "source": [
    "type_ = \"training\"\n",
    "Label_file, Input_file = ChooseDataset(type_)\n",
    "A = CreateDataset(Label_file, Input_file, 10, type_, None)\n",
    "X_train, y_train, info_train, word_to_index_training = A.DateGen()\n",
    "print(\"X shape: {}\\ny shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(info_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following: ['msr', 'cityu', 'as', 'pku']pku\n",
      "X shape: (1945, 10)\n",
      "y shape: (1945, 10, 4)\n",
      "{'MAXLEN': 10, 'VocabSize': 285201}\n"
     ]
    }
   ],
   "source": [
    "type_ = 'dev'\n",
    "Label_file, Input_file = ChooseDataset(type_)\n",
    "A = CreateDataset(Label_file, Input_file, 10, type_, word_to_index_training)\n",
    "X_dev, y_dev, info_dev, _ = A.DateGen()\n",
    "print(\"X shape: {}\\ny shape: {}\".format(X_dev.shape, y_dev.shape))\n",
    "print(info_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE SOME COSTANTS\n",
    "MAX_LENGTH = 88\n",
    "VOCAB_SIZE = info_dev['VocabSize']\n",
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "TO_BE_FOUND = info_dev['MAXLEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/1029 \n",
    "\n",
    "Explains Timedistributed in many-to-many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(vocab_size, embedding_size, hidden_size, TO_BE_FOUND):\n",
    "    print(\"Creating KERAS model\")\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Embedding(vocab_size, embedding_size, mask_zero=True, input_length = TO_BE_FOUND))\n",
    "    model.add(K.layers.LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 32)            9126432   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 256)           295936    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 4)             1028      \n",
      "=================================================================\n",
      "Total params: 9,423,396\n",
      "Trainable params: 9,423,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model = create_keras_model(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, TO_BE_FOUND)\n",
    "# Let's print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "cbk = K.callbacks.TensorBoard(\"logging/keras_model\")\n",
    "print(\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent = 10\n",
    "# size = int(len(X)/(1-percent))\n",
    "# X_train = X[:2000]\n",
    "# y_train = y[:2000]\n",
    "# dev_x = X[-300:]\n",
    "# dev_y = y[-300:]\n",
    "\n",
    "# X_dev, y_dev\n",
    "# X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train (19056, 10)\n",
      "y train (19056, 10, 4)\n",
      "X dev (1945, 10)\n",
      "y dev (1945, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train {}\".format(X_train.shape))\n",
    "print(\"y train {}\".format(y_train.shape))\n",
    "print(\"X dev {}\".format(X_dev.shape))\n",
    "print(\"y dev {}\".format(y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285196, 285180)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(i) for i in X_train]), max([max(i) for i in X_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(i) for i in X_train]), min([min(i) for i in X_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118066, 280596, 177867, ...,  66023, 243176, 174750], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 94686,  10200,  92011, ..., 207715, 185716,      0], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19056 samples, validate on 1945 samples\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "19056/19056 [==============================] - 92s 5ms/sample - loss: 1.0432 - acc: 0.5760 - val_loss: 1.0082 - val_acc: 0.5751\n",
      "Epoch 2/10\n",
      "19056/19056 [==============================] - 106s 6ms/sample - loss: 0.8839 - acc: 0.6363 - val_loss: 1.0065 - val_acc: 0.5902\n",
      "Epoch 3/10\n",
      "19056/19056 [==============================] - 100s 5ms/sample - loss: 0.8068 - acc: 0.6698 - val_loss: 0.9868 - val_acc: 0.5984\n",
      "Epoch 4/10\n",
      "19056/19056 [==============================] - 98s 5ms/sample - loss: 0.7347 - acc: 0.7001 - val_loss: 1.0187 - val_acc: 0.5927\n",
      "Epoch 5/10\n",
      "19056/19056 [==============================] - 92s 5ms/sample - loss: 0.6665 - acc: 0.7289 - val_loss: 1.0202 - val_acc: 0.5935\n",
      "Epoch 6/10\n",
      "19056/19056 [==============================] - 94s 5ms/sample - loss: 0.6000 - acc: 0.7600 - val_loss: 1.1607 - val_acc: 0.5871\n",
      "Epoch 7/10\n",
      "19056/19056 [==============================] - 94s 5ms/sample - loss: 0.5363 - acc: 0.7895 - val_loss: 1.1730 - val_acc: 0.5801\n",
      "Epoch 8/10\n",
      "19056/19056 [==============================] - 92s 5ms/sample - loss: 0.4795 - acc: 0.8164 - val_loss: 1.2166 - val_acc: 0.5720\n",
      "Epoch 9/10\n",
      "19056/19056 [==============================] - 100s 5ms/sample - loss: 0.4313 - acc: 0.8388 - val_loss: 1.2874 - val_acc: 0.5689\n",
      "Epoch 10/10\n",
      "19056/19056 [==============================] - 104s 5ms/sample - loss: 0.3852 - acc: 0.8580 - val_loss: 1.3423 - val_acc: 0.5766\n",
      "Training complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          shuffle=True, validation_data=(X_dev, y_dev), callbacks=[cbk]) \n",
    "print(\"Training complete.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../icwb2-data/testing/pku_test_simplified_InputFile.utf8'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose from the following: ['msr', 'cityu', 'as', 'pku']pku\n",
      "X shape: (1945, 10)\n",
      "y shape: (1945, 10, 4)\n",
      "{'MAXLEN': 10, 'VocabSize': 285201}\n"
     ]
    }
   ],
   "source": [
    "type_ = 'testing'\n",
    "Label_file, Input_file = ChooseDataset(type_)\n",
    "A = CreateDataset(Label_file, Input_file, 10, type_, word_to_index_training)\n",
    "X_test, y_test, info_test, _ = A.DateGen()\n",
    "print(\"X shape: {}\\ny shape: {}\".format(X_test.shape, y_test.shape))\n",
    "print(info_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating test...\n",
      "Test data: loss = 5.877808  accuracy = 3.18% \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating test...\")\n",
    "loss_acc = model.evaluate(X_test, y_test, verbose=3)\n",
    "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % (loss_acc[0], loss_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "W0406 23:24:06.817950 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:06.835707 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:06.858180 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:06.892616 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:06.931550 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "TensorBoard 1.13.1 at http://Ostyks-MBP.lan:6006 (Press CTRL+C to quit)\n",
      "W0406 23:24:06.967617 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:06.986957 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.007263 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.027457 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.051435 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.067175 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.077943 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.108265 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.120948 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.140505 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.144636 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.163632 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.168087 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.184298 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.193984 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.209026 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.210586 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.226408 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.228509 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.240813 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.242856 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.260382 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.264467 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.279888 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.285133 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.331204 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.344289 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "W0406 23:24:07.357336 123145545973760 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "W0406 23:24:07.360841 123145545973760 plugin_event_accumulator.py:302] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original file $\\rightarrow$ simplified Chinese\n",
    "- Input file $\\rightarrow$ used to feed Bi-LSTM model\n",
    "- Label file $\\rightarrow$ used to test the predictions\n",
    "\n",
    "TO DO: probably need a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1448/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
